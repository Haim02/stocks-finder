# from tradingview_screener import Query, Column
# import pandas as pd

# class ScreenerService:
#     @staticmethod
#     def get_candidates():
#         print("ğŸ” Scanning TradingView for candidates (Real-time)...")

#         try:
#             # 1. ×”×’×“×¨×ª ×”×©××™×œ×ª×”
#             q = Query().set_markets('america')

#             # 2. ×‘×—×™×¨×” ××¤×•×¨×©×ª ×©×œ ×”×¢××•×“×•×ª
#             q.select('name', 'close', 'volume', 'market_cap_basic')

#             # 3. ×”×¡×™× ×•× ×™× (×›××Ÿ ×”-WHERE ×©×”×™×” ×—×¡×¨ ×œ×š!)
#             q.where(
#                 Column('close') > 2.0,                     # ×¡×™× ×•×Ÿ ×§×¨×™×˜×™: ×¨×§ ×× ×™×•×ª ××¢×œ 2 ×“×•×œ×¨
#                 Column('close') > Column('SMA200'),        # ×× ×™×•×ª ×‘××’××ª ×¢×œ×™×” ×‘×œ×‘×“
#                 Column('relative_volume_10d_calc') > 1.2,  # ×•×•×œ×™×•× ×—×¨×™×’
#                 Column('type') == 'stock',                 # ×¨×§ ×× ×™×•×ª (×‘×œ×™ ×ª×¢×•×“×•×ª ×¡×œ)
#                 Column('average_volume_10d_calc') > 500000 # × ×–×™×œ×•×ª ×’×‘×•×”×”
#             )

#             # ××™×•×Ÿ: ×× ×—× ×• ×¨×•×¦×™× ××ª ××œ×• ×©×–×–×• ×”×›×™ ×—×–×§ ×”×™×•×
#             q.order_by('change', ascending=False)
#             q.limit(30)

#             # 4. ×§×‘×œ×ª × ×ª×•× ×™×
#             response = q.get_scanner_data()

#             if not response or len(response) < 2:
#                 print("âš ï¸ No results returned.")
#                 return []

#             rows = response[1]
#             tickers = []

#             # --- ×–×™×”×•×™ ×¤×•×¨××˜ (DataFrame ××• List) ---
#             if hasattr(rows, 'columns') and hasattr(rows, 'iloc'):
#                 # ××§×¨×” ×': ×§×™×‘×œ× ×• ×˜×‘×œ×” ×©×œ ×¤× ×“×¡
#                 if 'name' in rows.columns:
#                     tickers = rows['name'].tolist()
#                 elif 'ticker' in rows.columns:
#                     tickers = rows['ticker'].tolist()
#             else:
#                 # ××§×¨×” ×‘': ×§×™×‘×œ× ×• ×¨×©×™××” ×¨×’×™×œ×”
#                 for row in rows:
#                     ticker = None
#                     if isinstance(row, dict):
#                         ticker = row.get('name')
#                     elif hasattr(row, 'name'):
#                         ticker = row.name
#                     elif isinstance(row, list) and len(row) > 0:
#                         ticker = row[0]

#                     if ticker:
#                         tickers.append(ticker)

#             # --- × ×™×§×•×™ ×¡×•×¤×™ ---
#             clean_tickers = []
#             for t in tickers:
#                 if isinstance(t, str):
#                     # ×× ×§×” ×–×‘×œ ×›××• "NASDAQ:AAPL" -> "AAPL"
#                     clean_t = t.split(":")[-1].strip()
#                     # ××•×•×“× ×©×–×” ×œ× ×›×•×ª×¨×ª
#                     if clean_t.lower() not in ['name', 'ticker', 'close', 'volume', 'n/a']:
#                         clean_tickers.append(clean_t)

#             # ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª
#             final_tickers = list(set(clean_tickers))

#             print(f"âœ… Found {len(final_tickers)} valid stocks (> $2): {final_tickers}")
#             return final_tickers

#         except Exception as e:
#             print(f"âŒ Error in Screener Logic: {e}")
#             return []




# from tradingview_screener import Query, Column
# import pandas as pd

# class ScreenerService:
#     @staticmethod
#     def get_candidates():
#         print("ğŸ” Scanning TradingView for Early Breakout candidates (High Relative Vol)...")

#         try:
#             q = Query().set_markets('america')

#             q.select('name', 'close', 'volume', 'market_cap_basic')

#             q.where(
#                 Column('close') > 5.0,                         # ×¡×™× ×•×Ÿ ×× ×™×•×ª ×–×‘×œ (××¢×œ 5$)
#                 Column('close') > Column('SMA200'),            # ××’××ª ×¢×œ×™×” ×¨××©×™×ª (×—×•×‘×”)
#                 Column('average_volume_10d_calc') > 750000,    # × ×–×™×œ×•×ª ×’×‘×•×”×” ×××•×“
#                 Column('relative_volume_10d_calc') > 1.2,      # ×•×•×œ×™×•× ×’×‘×•×” ×‘-20% ××”×¨×’×™×œ (×¡×™××Ÿ ×œ×›× ×™×¡×ª ×›×¡×£)
#                 Column('type') == 'stock'
#             )

#             # ×©×™× ×•×™ ×§×¨×™×˜×™: ××™×•×Ÿ ×œ×¤×™ ×•×•×œ×™×•× ×™×—×¡×™ ×‘××§×•× ×œ×¤×™ ××—×•×– ×©×™× ×•×™!
#             # ×–×” ××‘×™× ×× ×™×•×ª ×©××ª×‘×©×œ ×‘×”×Ÿ ××”×œ×š, ×’× ×× ×”×Ÿ ×¢×•×“ ×œ× ×˜×¡×• 10%
#             q.order_by('relative_volume_10d_calc', ascending=False)
#             q.limit(35)

#             response = q.get_scanner_data()

#             if not response or len(response) < 2:
#                 print("âš ï¸ No results returned.")
#                 return []

#             rows = response[1]
#             tickers = []

#             if hasattr(rows, 'columns') and hasattr(rows, 'iloc'):
#                 if 'name' in rows.columns:
#                     tickers = rows['name'].tolist()
#                 elif 'ticker' in rows.columns:
#                     tickers = rows['ticker'].tolist()
#             else:
#                 for row in rows:
#                     ticker = None
#                     if isinstance(row, dict):
#                         ticker = row.get('name')
#                     elif hasattr(row, 'name'):
#                         ticker = row.name
#                     elif isinstance(row, list) and len(row) > 0:
#                         ticker = row[0]

#                     if ticker:
#                         tickers.append(ticker)

#             # × ×™×§×•×™
#             clean_tickers = []
#             for t in tickers:
#                 if isinstance(t, str):
#                     clean_t = t.split(":")[-1].strip()
#                     if clean_t.lower() not in ['name', 'ticker', 'close', 'volume', 'n/a']:
#                         clean_tickers.append(clean_t)

#             final_tickers = list(set(clean_tickers))
#             print(f"âœ… Found {len(final_tickers)} potential early-movers: {final_tickers}")
#             return final_tickers

#         except Exception as e:
#             print(f"âŒ Error in Screener Logic: {e}")
#             return []


# import requests
# from bs4 import BeautifulSoup

# class ScreenerService:
#     @staticmethod
#     def get_candidates_from_url(finviz_url):
#         """
#         ××§×‘×œ URL ×©×œ ××¡×š ×ª×•×¦××•×ª ×‘-Finviz ×•××—×–×™×¨ ×¨×©×™××ª ×˜×™×§×¨×™×.
#         """
#         headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
#         tickers = []

#         try:
#             print(f"ğŸŒ Fetching candidates from Finviz...")
#             response = requests.get(finviz_url, headers=headers)
#             soup = BeautifulSoup(response.content, 'html.parser')

#             # ×‘-Finviz ×”×˜×™×§×¨×™× × ××¦××™× ×‘×ª×•×š ×œ×™× ×§×™× ×¢× Class ×¡×¤×¦×™×¤×™
#             # ×”×¢×¨×”: ×”×§×œ××¡ ×™×›×•×œ ×œ×”×©×ª× ×•×ª, ×›×¨×’×¢ ×–×” screener-link-primary ××• ×“×•××”
#             links = soup.select('a.screener-link-primary')

#             for link in links:
#                 tickers.append(link.text.strip())

#             unique_tickers = list(set(tickers))
#             print(f"âœ… Found {len(unique_tickers)} candidates.")
#             return unique_tickers

#         except Exception as e:
#             print(f"âŒ Error fetching screener data: {e}")
#             return []


# import requests
# from bs4 import BeautifulSoup
# import time
# import random

# class ScreenerService:
#     @staticmethod
#     def get_candidates_from_url(finviz_url):
#         """
#         ××•×©×š ×˜×™×§×¨×™× ×-Finviz.
#         ×ª×™×§×•×Ÿ: ×”×¡×¨× ×• ××ª Accept-Encoding ×›×“×™ ×œ×§×‘×œ ×˜×§×¡×˜ ×§×¨×™× ×•×œ× ×“×—×•×¡.
#         """
#         headers = {
#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#             'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
#             'Accept-Language': 'en-US,en;q=0.5',
#             'Connection': 'keep-alive',
#             'Upgrade-Insecure-Requests': '1',
#             # ××—×§× ×• ××ª ×©×•×¨×ª ×”-Accept-Encoding ×›×“×™ ×œ×× ×•×¢ ××ª ×”×’'×™×‘×¨×™×©
#         }

#         tickers = []

#         try:
#             print(f"ğŸŒ Fetching candidates from Finviz...")

#             # ×”×©×”×™×™×” ×§×¦×¨×”
#             time.sleep(random.uniform(1, 2))

#             response = requests.get(finviz_url, headers=headers)

#             # ×•×™×“×•× ×§×™×“×•×“ ×˜×§×¡×˜ ×ª×§×™×Ÿ
#             response.encoding = 'utf-8'

#             if response.status_code != 200:
#                 print(f"âŒ Blocked/Error. Status: {response.status_code}")
#                 # ×¨×©×™××ª ×—×™×¨×•× ×‘××§×¨×” ×©×œ ×—×¡×™××”, ×›×“×™ ×©×ª×¨××” ×©×”××¢×¨×›×ª ×¢×•×‘×“×ª
#                 return ["NVDA", "TSLA", "AMD"]

#             soup = BeautifulSoup(response.text, 'html.parser')

#             # × ×™×¡×™×•×Ÿ ×œ××¦×•× ××ª ×”×˜×™×§×¨×™× ×‘-2 ×¡×•×’×™ ×”×œ×™× ×§×™× ×©-Finviz ××©×ª××© ×‘×”×
#             links = soup.select('a.screener-link-primary')
#             if not links:
#                 links = soup.select('a.screener-link')

#             for link in links:
#                 txt = link.text.strip()
#                 # ×‘×“×™×§×ª ×ª×§×™× ×•×ª: ×˜×™×§×¨ ×”×•× ×‘××•×ª×™×•×ª ×’×“×•×œ×•×ª, ×§×¦×¨, ×•×œ×œ× ××¡×¤×¨×™×
#                 if txt.isupper() and len(txt) <= 5 and txt.isalpha():
#                     tickers.append(txt)

#             unique_tickers = list(set(tickers))

#             if not unique_tickers:
#                 print("âš ï¸ Warning: No tickers found in HTML. Finviz structure might have changed.")
#                 # ×”×—×–×¨×ª ×¨×©×™××ª ×‘×¨×™×¨×ª ××—×“×œ ×›×“×™ ×œ× ×œ×¢×¦×•×¨ ××ª ×”×ª×”×œ×™×š
#                 return ["TSLA", "NVDA", "PLTR", "SOFI", "MARA"]

#             print(f"âœ… Found {len(unique_tickers)} candidates: {unique_tickers}")
#             return unique_tickers

#         except Exception as e:
#             print(f"âŒ Error fetching screener data: {e}")
#             return ["TSLA", "NVDA"] # Fallback



# import cloudscraper
# from bs4 import BeautifulSoup
# import time
# import random
# import re

# class ScreenerService:
#     @staticmethod
#     def get_candidates_from_url(finviz_url):
#         """
#         ××•×©×š ×˜×™×§×¨×™× ×-Finviz ×‘×××¦×¢×•×ª cloudscraper ×•×–×™×”×•×™ ×§×™×©×•×¨×™× ×—×›× (Regex).
#         """
#         scraper = cloudscraper.create_scraper(
#             browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
#         )

#         tickers = []

#         try:
#             print(f"ğŸŒ Fetching candidates from Finviz (Regex Method)...")
#             response = scraper.get(finviz_url)

#             if response.status_code != 200:
#                 print(f"âŒ Blocked/Error. Status: {response.status_code}")
#                 return ["TSLA", "NVDA", "AMD"]

#             soup = BeautifulSoup(response.text, 'html.parser')

#             # --- ×”×©×™× ×•×™ ×”×’×“×•×œ: ×—×™×¤×•×© ×œ×¤×™ ××‘× ×” ×”×§×™×©×•×¨ ×•×œ× ×œ×¤×™ ×¢×™×¦×•×‘ ---
#             # ×× ×—× ×• ××—×¤×©×™× ×›×œ ×œ×™× ×§ ×©××ª×—×™×œ ×‘- "quote.ashx?t="
#             # ×–×” ×¢×•×‘×“ ×ª××™×“, ×œ× ××©× ×” ×× Finviz ××©× ×™× ××ª ×”×¢×™×¦×•×‘
#             all_links = soup.find_all('a', href=True)

#             for link in all_links:
#                 href = link['href']

#                 # ×‘×“×™×§×” ×× ×”×œ×™× ×§ ××•×‘×™×œ ×œ×× ×™×”
#                 if "quote.ashx?t=" in href:
#                     # ×—×™×œ×•×¥ ×”×˜×™×§×¨ ××”×œ×™× ×§ ××• ××”×˜×§×¡×˜
#                     # ×“×•×’××” ×œ×œ×™× ×§: quote.ashx?t=AAPL&ty=c...
#                     # ×× ×—× ×• ×¨×•×¦×™× ×¨×§ ××ª ×”-AAPL

#                     # × × ×¡×” ×œ×§×—×ª ××ª ×”×˜×§×¡×˜ ×©×œ ×”×œ×™× ×§ ×§×•×“× (×‘×“×¨×š ×›×œ×œ ×–×” ×”×˜×™×§×¨)
#                     ticker_text = link.text.strip()

#                     # ×•×™×“×•× ×©×–×” × ×¨××” ×›××• ×˜×™×§×¨ (××•×ª×™×•×ª ×’×“×•×œ×•×ª, ×§×¦×¨)
#                     if ticker_text.isupper() and len(ticker_text) <= 5 and ticker_text.isalpha():
#                          tickers.append(ticker_text)
#                     else:
#                         # × ×™×¡×™×•×Ÿ ×—×™×œ×•×¥ ××ª×•×š ×”-URL ×× ×”×˜×§×¡×˜ ×œ× ×‘×¨×•×¨
#                         match = re.search(r't=([A-Z]+)', href)
#                         if match:
#                             tickers.append(match.group(1))

#             # ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª (×›×™ ×œ×¤×¢××™× ×™×© ×›××” ×œ×™× ×§×™× ×œ××•×ª×” ×× ×™×” ×‘×“×£)
#             unique_tickers = list(set(tickers))

#             if not unique_tickers:
#                 print("âš ï¸ Warning: No tickers found. Check if your filter yields 0 results.")
#                 print(f"Debug Title: {soup.title.string if soup.title else 'No Title'}")
#                 return ["TSLA", "NVDA"] # Fallback

#             print(f"âœ… Found {len(unique_tickers)} candidates: {unique_tickers}")
#             return unique_tickers

#         except Exception as e:
#             print(f"âŒ Error scraping: {e}")
#             return ["TSLA", "NVDA"]


import cloudscraper
from bs4 import BeautifulSoup
import re

class ScreenerService:
    @staticmethod
    def get_candidates_from_url(finviz_url):
        """
        ××•×©×š ×˜×™×§×¨×™× ×-Finviz ×‘×××¦×¢×•×ª cloudscraper ×•×–×™×”×•×™ ×§×™×©×•×¨×™× ×—×›× (Regex).
        ×’×¨×¡×” × ×§×™×™×” - ×œ×œ× ×× ×™×•×ª ×’×™×‘×•×™.
        """
        # ×”×’×“×¨×ª ×“×¤×“×¤×Ÿ ××–×•×™×£ ×œ×¢×§×™×¤×ª ×—×¡×™××•×ª
        scraper = cloudscraper.create_scraper(
            browser={'browser': 'chrome', 'platform': 'windows', 'desktop': True}
        )

        tickers = []

        try:
            print(f"ğŸŒ Fetching candidates from Finviz...")
            response = scraper.get(finviz_url)

            if response.status_code != 200:
                print(f"âŒ Blocked/Error. Status: {response.status_code}")
                return [] # ××™×Ÿ ×ª×•×¦××•×ª

            soup = BeautifulSoup(response.text, 'html.parser')

            # ×—×™×¤×•×© ×›×œ ×”×œ×™× ×§×™× ×©××•×‘×™×œ×™× ×œ×¢××•×“ ×× ×™×” (quote.ashx?t=...)
            all_links = soup.find_all('a', href=True)

            for link in all_links:
                href = link['href']

                if "quote.ashx?t=" in href:
                    ticker_text = link.text.strip()

                    # ×•×™×“×•× ×©×–×” ××›×Ÿ ×˜×™×§×¨ ×ª×§×™×Ÿ (××•×ª×™×•×ª ×’×“×•×œ×•×ª, ×§×¦×¨)
                    if ticker_text.isupper() and len(ticker_text) <= 5 and ticker_text.isalpha():
                         tickers.append(ticker_text)
                    else:
                        # ×’×™×‘×•×™: ×—×™×œ×•×¥ ×”×˜×™×§×¨ ××ª×•×š ×”-URL ×¢×¦××• ×× ×”×˜×§×¡×˜ ×œ× ×‘×¨×•×¨
                        match = re.search(r't=([A-Z]+)', href)
                        if match:
                            tickers.append(match.group(1))

            unique_tickers = list(set(tickers))

            if not unique_tickers:
                print("âš ï¸ No tickers found matching your criteria.")
                return []

            print(f"âœ… Found {len(unique_tickers)} candidates: {unique_tickers}")
            return unique_tickers

        except Exception as e:
            print(f"âŒ Error scraping: {e}")
            return []