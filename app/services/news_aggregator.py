# import feedparser
# from datetime import datetime, timedelta
# from dateutil import parser
# import pytz
# import re

# class NewsAggregator:
#     def __init__(self):
#         # ×¨×©×™××ª ×”××§×•×¨×•×ª ×”×›×™ ×—×–×§×™× (RSS ×¨×©××™×™×)
#         self.feeds = {
#             "GlobeNewswire": "https://www.globenewswire.com/RssFeed/subject/code/MERGER-ACQUISITION-NEWS?include10K=False",
#             "PR Newswire": "https://www.prnewswire.com/rss/news/all-news-releases",
#             "Benzinga": "https://feeds.benzinga.com/benzinga/news",
#             "FDA Updates": "https://www.fda.gov/about-fda/contact-fda/stay-informed/rss-feeds/press-releases/rss.xml",
#             "Yahoo Finance": "https://finance.yahoo.com/news/rssindex"
#         }

#         # ××™×œ×•×ª ××¤×ª×— ×œ×¡×™× ×•×Ÿ - × ×©××•×¨ ×¨×§ ××” ×©×‘×××ª ×—×©×•×‘
#         self.keywords = [
#             "fda", "approval", "cleared", "phase 3", "phase 2", # ×‘×™×•×˜×§
#             "merger", "acquisition", "agreement", "contract", "partnership", # ×¢×¡×§××•×ª
#             "guidance", "upgrade", "buy rating", "patent", "awarded", "earnings" # ×–×¨×–×™×
#         ]

#     def fetch_last_24h_news(self):
#         """××•×©×š ×—×“×©×•×ª ××›×œ ×”××§×•×¨×•×ª ××”-24 ×©×¢×•×ª ×”××—×¨×•× ×•×ª"""
#         print("ğŸŒ Aggregating global market news (Last 24h)...")

#         news_items = []
#         utc_now = datetime.now(pytz.utc)
#         one_day_ago = utc_now - timedelta(days=1)

#         for source, url in self.feeds.items():
#             try:
#                 feed = feedparser.parse(url)

#                 for entry in feed.entries:
#                     # ×˜×™×¤×•×œ ×‘×ª××¨×™×›×™× (×”××¨×” ×œ-UTC)
#                     try:
#                         if hasattr(entry, 'published'):
#                             pub_date = parser.parse(entry.published)
#                         elif hasattr(entry, 'updated'):
#                             pub_date = parser.parse(entry.updated)
#                         else:
#                             continue

#                         if pub_date.tzinfo is None:
#                             pub_date = pub_date.replace(tzinfo=pytz.utc)
#                         else:
#                             pub_date = pub_date.astimezone(pytz.utc)

#                         # ×¡×™× ×•×Ÿ ×–××Ÿ: ×¨×§ 24 ×©×¢×•×ª ××—×¨×•× ×•×ª
#                         if pub_date < one_day_ago:
#                             continue

#                         headline = entry.title
#                         link = entry.link

#                         # ×¡×™× ×•×Ÿ ×ª×•×›×Ÿ: ×¨×§ ×× ×™×© ××™×œ×•×ª ××¤×ª×— ××¢× ×™×™× ×•×ª
#                         if self._is_relevant(headline):
#                             ticker = self._extract_ticker(headline) or "GENERAL"

#                             news_items.append({
#                                 "source": source,
#                                 "ticker": ticker,
#                                 "headline": headline,
#                                 "url": link,
#                                 "published_at": pub_date.strftime("%Y-%m-%d %H:%M"),
#                                 "raw_date": pub_date # ×œ×©×™××•×© ×¤× ×™××™ ×•××™×•×Ÿ
#                             })

#                     except Exception as e:
#                         continue
#             except Exception as e:
#                 print(f"âŒ Error reading feed {source}: {e}")

#         # ××™×•×Ÿ ×”×—×“×©×•×ª ××”×—×“×© ×œ×™×©×Ÿ
#         news_items.sort(key=lambda x: x['raw_date'], reverse=True)
#         print(f"âœ… Found {len(news_items)} important news items.")
#         return news_items

#     def _is_relevant(self, text):
#         text_lower = text.lower()
#         for kw in self.keywords:
#             if kw in text_lower:
#                 return True
#         return False

#     def _extract_ticker(self, text):
#         """×× ×¡×” ×œ××¦×•× ×˜×™×§×¨ ×‘×ª×•×š ×¡×•×’×¨×™×™×, ×œ××©×œ (AAPL)"""
#         match = re.search(r'\(([A-Z]{2,5})\)', text)
#         if match:
#             return match.group(1)
#         return None



import feedparser
from datetime import datetime, timedelta
from dateutil import parser
import pytz
import re

class NewsAggregator:
    def __init__(self):
        # ××§×•×¨×•×ª ××™×“×¢ ×—×–×§×™× ×œ×‘×™×•×˜×§ ×•×”×•×“×¢×•×ª ×œ×¢×™×ª×•× ×•×ª
        self.feeds = {
            "GlobeNewswire": "https://www.globenewswire.com/RssFeed/subject/code/MERGER-ACQUISITION-NEWS?include10K=False",
            "PR Newswire": "https://www.prnewswire.com/rss/news/all-news-releases",
            "Benzinga": "https://feeds.benzinga.com/benzinga/news",
            "FDA Updates": "https://www.fda.gov/about-fda/contact-fda/stay-informed/rss-feeds/press-releases/rss.xml",
            "Yahoo Finance": "https://finance.yahoo.com/news/rssindex"
        }

        # ××™×œ×•×ª ××¤×ª×— ×›×œ×œ×™×•×ª ×œ×—×“×©×•×ª ×©×•×§
        self.general_keywords = [
            "merger", "acquisition", "agreement", "contract", "partnership",
            "guidance", "upgrade", "earnings"
        ]

        # ××™×œ×•×ª ××¤×ª×— ×¡×¤×¦×™×¤×™×•×ª ×œ"×¦×™×™×“ ×”×‘×™×•×˜×§"
        self.biotech_keywords = [
            "fda approval", "fda approved", "cleared by fda", # ××™×©×•×¨×™×
            "phase 3", "phase iii", "phase 4", "phase iv",    # ×©×œ×‘×™× ××ª×§×“××™×
            "primary endpoint", "clinical trial results",     # ×ª×•×¦××•×ª × ×™×¡×•×™
            "fast track designation", "orphan drug",          # ×¨×’×•×œ×¦×™×” ×—×™×•×‘×™×ª
            "pdufa"                                           # ×ª××¨×™×š ×™×¢×“ ×œ××™×©×•×¨
        ]

    def fetch_last_24h_news(self):
        """××•×©×š ×—×“×©×•×ª ×›×œ×œ×™×•×ª (×›××• ×©×™×© ×œ×š ×”×™×•×)"""
        return self._scan_feeds(mode="general")

    def find_biotech_opportunities(self):
        """
        ×¤×•× ×§×¦×™×” ×—×“×©×”: ××—×–×™×¨×” ×¨×©×™××” ×©×œ ×˜×™×§×¨×™× (×¡×™××•×œ×™×) ×©×œ ×× ×™×•×ª ×‘×™×•×˜×§
        ×©×™×© ×œ×”×Ÿ ×—×“×©×•×ª ××¨×¢×™×©×•×ª ××”×™×××” ×”××—×¨×•× ×”.
        """
        print("ğŸ§¬ Scanning for Biotech/FDA opportunities...")
        news_items = self._scan_feeds(mode="biotech")

        biotech_tickers = []
        for item in news_items:
            # ×× ××¦×× ×• ×˜×™×§×¨ ×‘×ª×•×š ×”×—×“×©×” - ×–×• ×”×–×“×× ×•×ª!
            if item['ticker'] and item['ticker'] != "GENERAL":
                biotech_tickers.append(item['ticker'])

        # ×”×¡×¨×ª ×›×¤×™×œ×•×™×•×ª (×œ××©×œ ××•×ª×” ×× ×™×” ×”×•×¤×™×¢×” ×‘-2 ××ª×¨×™×)
        unique_tickers = list(set(biotech_tickers))
        if unique_tickers:
            print(f"ğŸ§¬ Found {len(unique_tickers)} Biotech stocks with major news: {unique_tickers}")
        return unique_tickers

    def _scan_feeds(self, mode="general"):
        """×¤×•× ×§×¦×™×” ×¤× ×™××™×ª ×œ×¡×¨×™×§×ª ×”×¤×™×“×™× ×œ×¤×™ ××¦×‘"""
        news_items = []
        utc_now = datetime.now(pytz.utc)
        one_day_ago = utc_now - timedelta(days=1)

        # ×”×’×“×¨×ª ××–×•×¨×™ ×–××Ÿ ×œ×ª×™×§×•×Ÿ ×”××–×”×¨×”
        tzinfos = {
            "EST": -18000, "EDT": -14400, "CST": -21600, "CDT": -18000,
            "PST": -28800, "PDT": -25200
        }

        keywords = self.biotech_keywords if mode == "biotech" else self.general_keywords

        for source, url in self.feeds.items():
            try:
                feed = feedparser.parse(url)
                for entry in feed.entries:
                    try:
                        # ×–×™×”×•×™ ×ª××¨×™×š
                        if hasattr(entry, 'published'):
                            pub_date = parser.parse(entry.published, tzinfos=tzinfos)
                        elif hasattr(entry, 'updated'):
                            pub_date = parser.parse(entry.updated, tzinfos=tzinfos)
                        else:
                            continue

                        if pub_date.tzinfo is None:
                            pub_date = pub_date.replace(tzinfo=pytz.utc)
                        else:
                            pub_date = pub_date.astimezone(pytz.utc)

                        if pub_date < one_day_ago:
                            continue

                        headline = entry.title
                        link = entry.link

                        # ×‘×“×™×§×ª ××™×œ×•×ª ××¤×ª×—
                        if self._is_relevant(headline, keywords):
                            ticker = self._extract_ticker(headline) or "GENERAL"

                            news_items.append({
                                "source": source,
                                "ticker": ticker,
                                "headline": headline,
                                "url": link,
                                "published_at": pub_date.strftime("%Y-%m-%d %H:%M"),
                                "raw_date": pub_date
                            })

                    except Exception:
                        continue
            except Exception:
                pass

        news_items.sort(key=lambda x: x['raw_date'], reverse=True)
        return news_items

    def _is_relevant(self, text, keywords):
        text_lower = text.lower()
        for kw in keywords:
            if kw in text_lower:
                return True
        return False

    def _extract_ticker(self, text):
        """
        ××—×œ×¥ ×˜×™×§×¨×™× ××¤×•×¨××˜×™× × ×¤×•×¦×™×:
        (NASDAQ: AAPL), (NYSE: T), (AAPL)
        """
        # × ×™×¡×™×•×Ÿ 1: ×¤×•×¨××˜ ××œ× (NASDAQ: XXXX)
        match = re.search(r'\((?:NASDAQ|NYSE|AMEX):\s?([A-Z]{2,5})\)', text, re.IGNORECASE)
        if match:
            return match.group(1)

        # × ×™×¡×™×•×Ÿ 2: ×¤×•×¨××˜ ×§×¦×¨ (XXXX) - ××‘×œ ×¨×§ ××•×ª×™×•×ª ×’×“×•×œ×•×ª ×•××§×¤×™×
        # × ×–×”×¨ ×œ× ×œ×ª×¤×•×¡ ××™×œ×™× ×¨×’×™×œ×•×ª ×‘×¡×•×’×¨×™×™× ×›××• (Phase 3)
        match_simple = re.search(r'\s\(([A-Z]{2,5})\)', text)
        if match_simple:
            # ×¡×™× ×•×Ÿ ×¨×¢×©×™×: ××•×•×“× ×©×–×” ×œ× ××™×œ×” × ×¤×•×¦×”
            candidate = match_simple.group(1)
            if candidate not in ["FDA", "USA", "CEO", "CFO", "YOY", "QOQ", "USD"]:
                return candidate

        return None